{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q - Learning for the TSP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the Travelling Salesman Problem (TSP) with n cities. [...] Here, an instance of TSP will be fully described by a matrix $D$ of size $n \\times n$, where $n$ is the number of cities and $D[i,j]$ is the distance between city $i$ and city $j$.\n",
    "\n",
    "Here is a method to find a solution (of good quality but not always optimal) using the Q learning method. An agent will be travelling trough the cities, going through each city exactly one time, receiving rewards at each stop and when he finishes a circuit. Over the iteration, the agent will explore the space ( [explain $\\epsilon$ - greedy] ) and learn the Q matrix which will in the end provide a way to compute a good solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent will want to maximize his reward. The reward matrix will be such that the agents is reward $ - D[i,j]$ when he travels from city $i$ to city $j$ and $G = 100$ (arbitrary maybe change it ?) when he finishes a circuit. Since we don't allow the agent to go through the same city twice, we will set $D[i,i] = \\infty $.\n",
    "\n",
    "The $Q$ matrix should contain a cell for every pair $state \\times action$, here the state and the action can both be represented by the cities. This matrix will be \"dynamic\" because it will change after each action (because once a city is visited, it is no longer part of the set of possible future actions). This is just for ease of implementation. [Explain a little bit]. \n",
    "\n",
    "One iteration of the algorithm will consist in the agent building one path through the cities. During one iteration, we will have a parameter $\\epsilon$ controlling the $\\epsilon-greedy$ search, that is every time the agent finds himself in city $i$, he will go to a random city (among those not yet visited) with probability $\\epsilon$ and go the city that yield the highest future reward according to the Q matrix with probability $1-\\epsilon$. After each iteration, $\\epsilon$ will be decreased such that $\\epsilon_{t+1} = \\epsilon_{t} \\cdot (1-\\lambda)$, with $\\lambda \\in (0,1)$\n",
    "\n",
    "Each time the agent arrives to a city, we update one cell of the Q matrix according to :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q^{new}(s_{t},a_{t})\\leftarrow \\underbrace {Q(s_{t},a_{t})} _{\\text{old value}}+\\underbrace {\\alpha } _{\\text{learning rate}}\\cdot \\overbrace {{\\bigg (}\\underbrace {\\underbrace {r_{t}} _{\\text{reward}}+\\underbrace {\\gamma } _{\\text{discount factor}}\\cdot \\underbrace {\\max _{a}Q(s_{t+1},a)} _{\\text{estimate of optimal future value}}} _{\\text{new value (temporal difference target)}}-\\underbrace {Q(s_{t},a_{t})} _{\\text{old value}}{\\bigg )}} ^{\\text{temporal difference}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(formula from wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearnTSP:\n",
    "    def __init__(self, dist):\n",
    "        self.len_path = dist.shape[0]\n",
    "        self.dist = dist\n",
    "        #reward matrix, and Q matrix under the form of dictionnaries\n",
    "        #--------- should be optimized --------- #\n",
    "        start = 0 \n",
    "        end = 0 \n",
    "        rewards = {}\n",
    "        q = {}\n",
    "        states = [\"start\"] + [i for i in range(1, self.len_path)] + [\"end\"]\n",
    "        for s in states:\n",
    "            for n in states:\n",
    "                if s == n: \n",
    "                    q[(s,n)] = -np.inf\n",
    "                    continue\n",
    "                q[(s,n)]=0\n",
    "                s_i, n_i = s, n\n",
    "                if type(s) == str:\n",
    "                    s_i = 0\n",
    "                if type(n) == str:\n",
    "                    n_i = 0\n",
    "                if s_i == n_i: \n",
    "                    continue    \n",
    "\n",
    "                rewards[(s_i, n_i)] = -dist[s_i, n_i]\n",
    "                if n_i == 0:\n",
    "                    rewards[(s_i, n_i)] = 100 - dist[s_i, n_i]  \n",
    "        self.rewards = rewards\n",
    "        self.q = q\n",
    "    \n",
    "    \n",
    "\n",
    "    def follow_path(self, q_temp):\n",
    "        path = ['start']\n",
    "        while 'end' not in path:\n",
    "            possible_next = set([i for i in range(1, self.len_path)]) - set(path)\n",
    "            if possible_next == set():\n",
    "                path.append('end')\n",
    "                continue\n",
    "            next_state = list(possible_next)[0]\n",
    "            r = q_temp[(path[-1], next_state)]\n",
    "            for ele in possible_next:\n",
    "                if q_temp[(path[-1], ele)] > r:\n",
    "                    next_state = ele\n",
    "                    r = q_temp[(path[-1], ele)]\n",
    "            path.append(next_state)\n",
    "        return [0 if x in ['start', 'end'] else x for x in path]\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_cost(self, path):\n",
    "        c = 0\n",
    "        for i in range(1, len(path)):\n",
    "            c += self.dist[path[i-1], path[i]]\n",
    "        return c\n",
    "\n",
    "    def qlearning(self, epsilon, gamma, lr, lbda, epochs = 100):\n",
    "        start = 0\n",
    "        eps = epsilon\n",
    "        q_learn = self.q.copy()\n",
    "        for t in range(epochs):\n",
    "            if t%(epochs/10) == 0:\n",
    "                print(\"Iteration :\", t, \"||\", \"epsilon = \", round(eps * (1-lbda), 2), \"||\", \"Current cost :\", self.get_cost(self.follow_path(q_learn)))\n",
    "\n",
    "            current_state = start\n",
    "            current_path = [start]\n",
    "            eps = eps * (1-lbda)\n",
    "            while len(current_path) < self.len_path+1:\n",
    "                if len(current_path) == self.len_path:\n",
    "                    current_path.append(0)\n",
    "                    #do \n",
    "\n",
    "                    #\n",
    "                    continue\n",
    "                current_state = current_path[-1]\n",
    "                possible_next = set([i for i in range(1, self.len_path)]) - set(current_path)\n",
    "                u = np.random.random()\n",
    "                if u < eps:\n",
    "                    next_state = np.random.choice(list(possible_next))\n",
    "                else:\n",
    "                    key_next = [(current_state, n) for n in possible_next]\n",
    "                    next_state = list(possible_next)[0]\n",
    "                    r = self.rewards[(current_state, next_state)]\n",
    "                    for e in key_next:\n",
    "                        if self.rewards[e] > r:\n",
    "                            next_state = e[1]\n",
    "                            r = self.rewards[e]\n",
    "                current_path.append(next_state)\n",
    "                #updating Q\n",
    "                c_s, n_s, c_s_i, n_s_i = current_state, next_state, current_state, next_state\n",
    "                if current_state == 0:\n",
    "                    c_s_i = 'start'\n",
    "                if next_state == 0:\n",
    "                    n_s_i = 'end'\n",
    "                possible_next = set([i for i in range(1, self.len_path)]) - set(current_path) - set([next_state])\n",
    "                if possible_next == set():\n",
    "                    continue\n",
    "\n",
    "                max_r_n = list(possible_next)[0]\n",
    "                m_r = q_learn[(c_s_i, n_s_i)]\n",
    "                for ele in possible_next.union(set([\"end\"])):\n",
    "                    if q_learn[(c_s_i, ele)] > m_r:\n",
    "                        max_r_n = ele\n",
    "                        m_r = q_learn[(c_s_i, ele)] \n",
    "\n",
    "                q_learn[(c_s_i, n_s_i)] = q_learn[(c_s_i, n_s_i)] + lr * ((self.rewards[(c_s, n_s)] + gamma * m_r - q_learn[(c_s_i, n_s_i)]))\n",
    "        return q_learn\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix_15 = np.loadtxt(\"tsp_15_291.txt\") #15 cities and min cost = 291\n",
    "dist_matrix_26 = np.loadtxt(\"tsp_26_937.txt\")\n",
    "dist_matrix_17 = np.loadtxt(\"tsp_17_2085.txt\")\n",
    "dist_matrix_42 = np.loadtxt(\"tsp_42_699.txt\")\n",
    "dist_matrix_48 = np.loadtxt(\"tsp_48_33523.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = QLearnTSP(dist_matrix_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 0 || epsilon =  1.0 || Current cost : 817.0\n",
      "Iteration : 400 || epsilon =  0.96 || Current cost : 295.0\n",
      "Iteration : 800 || epsilon =  0.92 || Current cost : 291.0\n",
      "Iteration : 1200 || epsilon =  0.89 || Current cost : 291.0\n",
      "Iteration : 1600 || epsilon =  0.85 || Current cost : 291.0\n",
      "Iteration : 2000 || epsilon =  0.82 || Current cost : 291.0\n",
      "Iteration : 2400 || epsilon =  0.79 || Current cost : 291.0\n",
      "Iteration : 2800 || epsilon =  0.76 || Current cost : 291.0\n",
      "Iteration : 3200 || epsilon =  0.73 || Current cost : 291.0\n",
      "Iteration : 3600 || epsilon =  0.7 || Current cost : 291.0\n"
     ]
    }
   ],
   "source": [
    "Q_15 = solver.qlearning(1, 0.88, 0.1, 0.0001, 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = QLearnTSP(dist_matrix_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 0 || epsilon =  1.0 || Current cost : 4517.0\n",
      "Iteration : 1000 || epsilon =  0.9 || Current cost : 2199.0\n",
      "Iteration : 2000 || epsilon =  0.82 || Current cost : 2199.0\n",
      "Iteration : 3000 || epsilon =  0.74 || Current cost : 2199.0\n",
      "Iteration : 4000 || epsilon =  0.67 || Current cost : 2187.0\n",
      "Iteration : 5000 || epsilon =  0.61 || Current cost : 2187.0\n",
      "Iteration : 6000 || epsilon =  0.55 || Current cost : 2187.0\n",
      "Iteration : 7000 || epsilon =  0.5 || Current cost : 2187.0\n",
      "Iteration : 8000 || epsilon =  0.45 || Current cost : 2187.0\n",
      "Iteration : 9000 || epsilon =  0.41 || Current cost : 2187.0\n"
     ]
    }
   ],
   "source": [
    "Q_17 = solver.qlearning(1, 0.88, 0.1, 0.0001, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = QLearnTSP(dist_matrix_26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 0 || epsilon =  1.0 || Current cost : 1159.0\n",
      "Iteration : 1000 || epsilon =  0.37 || Current cost : 1050.0\n",
      "Iteration : 2000 || epsilon =  0.14 || Current cost : 1050.0\n",
      "Iteration : 3000 || epsilon =  0.05 || Current cost : 1050.0\n",
      "Iteration : 4000 || epsilon =  0.02 || Current cost : 1050.0\n",
      "Iteration : 5000 || epsilon =  0.01 || Current cost : 1050.0\n",
      "Iteration : 6000 || epsilon =  0.0 || Current cost : 1050.0\n",
      "Iteration : 7000 || epsilon =  0.0 || Current cost : 1050.0\n",
      "Iteration : 8000 || epsilon =  0.0 || Current cost : 1050.0\n",
      "Iteration : 9000 || epsilon =  0.0 || Current cost : 1050.0\n"
     ]
    }
   ],
   "source": [
    "Q_26 = solver.qlearning(1, 0.88, 0.2, 0.001, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = QLearnTSP(dist_matrix_42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 0 || epsilon =  1.0 || Current cost : 962.0\n",
      "Iteration : 1000 || epsilon =  0.9 || Current cost : 943.0\n",
      "Iteration : 2000 || epsilon =  0.82 || Current cost : 954.0\n",
      "Iteration : 3000 || epsilon =  0.74 || Current cost : 955.0\n",
      "Iteration : 4000 || epsilon =  0.67 || Current cost : 955.0\n",
      "Iteration : 5000 || epsilon =  0.61 || Current cost : 955.0\n",
      "Iteration : 6000 || epsilon =  0.55 || Current cost : 955.0\n",
      "Iteration : 7000 || epsilon =  0.5 || Current cost : 955.0\n",
      "Iteration : 8000 || epsilon =  0.45 || Current cost : 955.0\n",
      "Iteration : 9000 || epsilon =  0.41 || Current cost : 955.0\n"
     ]
    }
   ],
   "source": [
    "Q_42 = solver.qlearning(1, 0.8, 0.1, 0.0001, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = QLearnTSP(dist_matrix_48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 0 || epsilon =  1.0 || Current cost : 151722.0\n",
      "Iteration : 400 || epsilon =  0.96 || Current cost : 48117.0\n",
      "Iteration : 800 || epsilon =  0.92 || Current cost : 41718.0\n",
      "Iteration : 1200 || epsilon =  0.89 || Current cost : 40985.0\n",
      "Iteration : 1600 || epsilon =  0.85 || Current cost : 40985.0\n",
      "Iteration : 2000 || epsilon =  0.82 || Current cost : 40985.0\n",
      "Iteration : 2400 || epsilon =  0.79 || Current cost : 40985.0\n",
      "Iteration : 2800 || epsilon =  0.76 || Current cost : 40985.0\n",
      "Iteration : 3200 || epsilon =  0.73 || Current cost : 40551.0\n",
      "Iteration : 3600 || epsilon =  0.7 || Current cost : 40551.0\n"
     ]
    }
   ],
   "source": [
    "Q_48 = solver.qlearning(1, 0.8, 0.1, 0.0001, 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
